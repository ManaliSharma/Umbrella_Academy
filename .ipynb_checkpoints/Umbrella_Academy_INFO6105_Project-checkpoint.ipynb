{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Social Butterfly - Umbrella Academy</p>\n",
    "![title](Umbrella_Academy_INFO6105_Project\\Images\\Title_Images\\social-butterfly.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> 1.0 Abstract </p> <a id='abstract'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social Butterfly is a social media engagement software project which is a part of the NEU AI Skunkworks Project team. We propose to create a software that performs the task of content optimization and then publishes the same optimized content on one or more social media platform (viz. Twitter, Slack, Instagram etc.).\n",
    "\n",
    "Publishing content on social media is the fastest way to reach audience such that the content that should be posted and the timing at which it should be posted is of prime importance. Finding new content is easier these days but finding content with good data quality is incredibly hard. Also, then uniqueness of content poses a huge problem, because generating unique content in this era of duplication is like finding a needle in a haystack. And let's say even when you are able to create it and that doesn’t get popular and is not ranked well, is a wasted expense. However, producing a blog post which becomes popular and ranks highly is a wise investment. Repeating the same process will give you a consistent basis for a serious competitive advantage. Most people feel that optimizing content manually is a tedious and time consuming process. But it doesn’t have to be that way.  \n",
    "\n",
    "And that's how our idea builds, we are aiming to create content with good data quality in a given time frame which can become popular as it gets posted and majorly adds a x-factor to one's social media handle. We have tried incorporating content optimization into content creation workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PS-** We have done this project in 3 parts, and all 3 parts are divided into seperate notebooks. And these individual notebooks contain the whole code and documentation of the entire part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Collecting Metadata**:- In lay man's term for creating content on particular topic we need to know what is the present basis of it. And for the same reason this notebook comes handy. In this notebook we tackle with the task of Metadata collection . Metadata is nothing but the keywords which shows relevance of a topic and by which searching about the topic becomes easier. We have chosen to collect metadata in form of hashtags from twitter as it is easy to scrape data from twitter with the help of Python library like Tweepy. We save the scraped tweets in json file and then convert it to a csv file. This csv file is then loaded as a pandas dataframe, and we extract hashtags from the tweets. Then we plot these hashtags with their count to see their frequency of occurence. This gives us some good idea as to which hashtags should we use while scraping content from twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **LSTM Modeling**:- Now that we have scrapped metadata in form of hashtags in Collecting Metadata (Part 1) ,we will be scrapping relevant data using metadata i.e hashtags from twitter by means of a real time streaming listener. For this data being scrapped we won't be using retweets and the conditional requirement is that dataset should be large enough in order fo us to build the model.And then we will be employing various machine learning algorithm for content generation. Mostly we will be using LSTM for generation if content and we will be tweaking the same to get a better model which yields better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Sentiment Analysis**:- Generating content was a primary task for this project , but only creation of it is not enough. We also need to know how it influences people and creates what sort of impact on social media networks. And that's when sentiment analysis comes in play. In this notebook , we have performed sentiment analysis on the real time tweets (newly generated not the one we did our modeling on in previous notebook), that we collected using stream listener. And we found sentiment for each tweet and compared the tweets on the basis of the sentiments recieved. We used textblob to find the context of the inputted document we formed using tweets . After this we used a gensim word2vec model to find words with similar meaning and then we employed LSTM modeling for finding the sentiment of inputted texts that is being employed on text generated (extracted from part 2 i.e LSTM modeling of the project) in order to see what influence it will have on people. The following notebook can also be used for finding influencers that has most impact on twitter over all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our project aims to create a evolving model which optimizes the content for social platform  by experimenting on other available platforms like Twitter , Instagram etc. We have done so by using various machine learning techniques like LSTM etc and open source libraries like textblob, gensim, word2vec to make our model.**\n",
    "\n",
    "Relevant information about each topic can be found in each notebook which is specified in the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Index </p>\n",
    "- # 1 [Abstract](#abstract)\n",
    "- # 2 [Collecting Metadata](./Umbrella_Academy_INFO6105_Project/Umbrella_Academy_INFO6105_Collecting_Metadata.ipynb)\n",
    "- # 3 [LSTM Modeling](./Umbrella_Academy_INFO6105_Project/Umbrella_Academy_INFO6105_LSTM_Model.ipynb)\n",
    "- # 4 [Sentiment Analysis](./Umbrella_Academy_INFO6105_Project/Umbrella_Academy_INFO6105_Sentiment_Analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Conclusion </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. DataScience, AI, ML were the hashtags that we put in for scraping As we can see from the graph above AI, ML, machinelearning, bigdata were the the top most hashtags, which we will be using to scrap data from twitter\n",
    "\n",
    "2. We are not interested in the most accurate (classification accuracy) model of the training dataset. The model is the one that predicts each character and word in the training dataset perfectly. Instead we are interested in a generalization of the dataset that minimizes the chosen loss function. We are seeking a balance between generalization and overfitting but short of memorization.\n",
    "\n",
    "3. Still working on BLEU and Meteor score though we are able to calculate it , it gives us a low value because the text that is being generated as of now is a bit gibberish and will yield a low value, ideally the score for BLEU should be between 0.0 to 1.0 and we got a value of 1.276854e-89, which is low , so if modeling is tweaked it may yield good results, this is still being worked upon\n",
    "\n",
    "4. As of now we are able to generate text, which seems much more gibberish but is making sense in some parts, the text generated can be tuned by adding different dense layers , building complex neuron network and by increasing epochs and decreasing the batch size.\n",
    "\n",
    "5. Model can also perform well , if we take a large vocab size and our input seed lenght is good enough, although time consuming, but output can vary differently if we experiment it in that way.\n",
    "\n",
    "6. Adding weights to model with better accuracy results results in making a model (deeper and denser one better), we can keep on iterating that ways.\n",
    "\n",
    "7. The collected data can be tested upon to find its sentiment by our third project notebook (sentiment analysis part 3)\n",
    "\n",
    "8. As we scrapped the data for given list of hashtags, we found that the sentiment is mostly positive or neutral than negative.\n",
    "\n",
    "9. When we employed text blob, it was giving us 70% accuracy in aspect of knowing the data.\n",
    "\n",
    "10. We employed Gensim, and with it's help we were able to find similar meaning words i.e. 'machinelearning' returned python, datascience, deep learing and so on.\n",
    "\n",
    "11. The model on the test set of 3 class sentiment classification provides a result of 59.5 %accuracy.\n",
    "\n",
    "12. We modeled the data and achieved 50% accuracy, but that was not the aim, we are seeking a balance between generalization and on overfitting and getting a 50% accuracy is also a good enough measure.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
