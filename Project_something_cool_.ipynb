{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "twitter_cred = dict()\n",
    "\n",
    "twitter_cred['CONSUMER_KEY'] = '17XHiftb4Iu5hVAIh4NacWj9S'\n",
    "twitter_cred['CONSUMER_SECRET'] = '8xAXhFsfnkiFjSFO5THqRO5J8x8lKSzC9J2GNPUSY7ZPb5z6Qv'\n",
    "twitter_cred['ACCESS_KEY'] = '1392379572-FInzXaziEqyNCGc8jqtU9LwnFFlHYRpjoihZpiX'\n",
    "twitter_cred['ACCESS_SECRET'] = 'JBDveSiUU6aXpwx8QkxYhXloDr9DyHn7ocqrqgWqwqctw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twitter_credentials.json', 'w') as secret_info:\n",
    "    json.dump(twitter_cred, secret_info, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twitter_credentials.json') as cred_data:\n",
    "    info = json.load(cred_data)\n",
    "    consumer_key = info['CONSUMER_KEY']\n",
    "    consumer_secret = info['CONSUMER_SECRET']\n",
    "    access_key = info['ACCESS_KEY']\n",
    "    access_secret = info['ACCESS_SECRET']\n",
    "\n",
    "# Create the api endpoint\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tweet_scrapped():\n",
    "    maximum_number_of_tweets_to_be_extracted = \\\n",
    "    tweet_number=int(input('Enter the number of tweets that you want to extract- '))\n",
    "    return tweet_number\n",
    "def Hashtag_used():\n",
    "    hashtag=str(input('Enter the hashtag you want to scrape- '))\n",
    "    return hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_tweets(hashtag,tweet_number):\n",
    "    all_the_tweets = [] \n",
    "    with open('output_json.json', 'w') as outfile:\n",
    "        \n",
    "        for tweet in tweepy.Cursor(api.search, q='#' + hashtag,rpp=100).items(tweet_number):\n",
    "            if (not tweet.retweeted) :\n",
    "                json.dump(tweet._json, outfile, indent=2) \n",
    "                outfile.write('\\n')\n",
    "                all_the_tweets.append(tweet)\n",
    "    #with open('tweets_with_hashtag_' + hashtag + '.txt', 'a') as the_file:\n",
    "        #the_file.write(str(tweet.text.encode('utf-8')) + '\\n')\n",
    "    #data=process_or_store(tweet._json)\n",
    "    #get_tweets()\n",
    "      \n",
    "\n",
    "    print ('Extracted ' + str(tweet_number) + ' tweets with hashtag #' + hashtag)\n",
    "    #print(all_the_tweets)\n",
    "    return all_the_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_tweets(tweets_scrapped):\n",
    "    \n",
    "    outtweets = [[tweet.id_str, tweet.created_at,tweet.text.encode('utf-8').decode(\"utf-8\"),tweet.user.name,tweet.user.screen_name,tweet.user.friends_count,tweet.user.followers_count,tweet.retweet_count,tweet.favorite_count] for tweet in tweets_scrapped]\n",
    "    return outtweets\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_tweets(outtweets,hashtag):\n",
    "    tweet_csv_name=' tweets with hashtag #' + hashtag+'.csv'\n",
    "    with open(tweet_csv_name, 'w', encoding='utf8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['id', 'created_at', 'text','user','screen_name','friends_count','followers_count','retweet_count','favorite_count'])\n",
    "    \n",
    "        writer.writerows(outtweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of tweets that you want to extract- 10\n"
     ]
    }
   ],
   "source": [
    "tweet_number=int(tweet_scrapped())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the hashtag you want to scrape- DataScience\n"
     ]
    }
   ],
   "source": [
    "hashtag=str(Hashtag_used())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 10 tweets with hashtag #DataScience\n"
     ]
    }
   ],
   "source": [
    "tweets_scrapped=scrap_tweets(hashtag,tweet_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "outtweets=out_tweets(tweets_scrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_tweets(outtweets,hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\" u\"\\U0001F600-\\U0001F64F\"  # emoticons \n",
    "                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                u\"\\U00002702-\\U000027B0\"\n",
    "                                u\"\\U000024C2-\\U0001F251\"\n",
    "                                u\"\\U0001f926-\\U0001f937\"\n",
    "                                u'\\U00010000-\\U0010ffff'\n",
    "                                u\"\\u200d\"\n",
    "                                u\"\\u2640-\\u2642\"\n",
    "                                u\"\\u2600-\\u2B55\"\n",
    "                                u\"\\u23cf\"\n",
    "                                u\"\\u23e9\"\n",
    "                                u\"\\u231a\"\n",
    "                                u\"\\u3030\"\n",
    "                                u\"\\ufe0f\"\n",
    "                                \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe(hashtag):\n",
    "    tweet_csv_name=' tweets with hashtag #' + hashtag+'.csv'\n",
    "    dataset=pd.read_csv(tweet_csv_name,encoding=\"ISO-8859-1\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset=dataframe('DataScience')\n",
    "#dataset_1=dataframe('Machinelearning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 9 columns):\n",
      "id                 10 non-null int64\n",
      "created_at         10 non-null object\n",
      "text               10 non-null object\n",
      "user               10 non-null object\n",
      "screen_name        10 non-null object\n",
      "friends_count      10 non-null int64\n",
      "followers_count    10 non-null int64\n",
      "retweet_count      10 non-null int64\n",
      "favorite_count     10 non-null int64\n",
      "dtypes: int64(5), object(4)\n",
      "memory usage: 800.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = {\"HASHTAG\": r\"(#[\\w\\d]+)\"}\n",
    "regexp = dict((key, re.compile(value)) for key, value in regexp.items())\n",
    "\n",
    "def remove_hashtags(s):\n",
    "    return re.findall(regexp[\"HASHTAG\"], s)\n",
    "\n",
    "def extract_hash_tags(s):\n",
    "    my_set=set(part[1:] for part in s.split() if part.startswith('#'))\n",
    "    my_list = list(my_set)\n",
    "    return(my_list)\n",
    "\n",
    "def remove_hashtags_(x):\n",
    "    if '#' in x:\n",
    "        return str(x.replace('#',''))\n",
    "        \n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,'')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "        \n",
    "def column_text(dataset):\n",
    "    #dataset['text_1']=dataset['text'].apply(lambda row: row.encode('utf-8'))\n",
    "    dataset['text_1']=dataset['text'].apply(lambda row: remove_emoji(row))\n",
    "    dataset['Hashtags']=dataset['text'].apply(lambda row: remove_hashtags(row))\n",
    "    list_trial=dataset['text'].apply(lambda row: remove_hashtags(row))\n",
    "    #dataset['Hashtags']=dataset['text'].apply(lambda row: extract_hash_tags(row))\n",
    "    dataset['TextNoHashtags']=dataset['text'].apply(remove_hashtags_)\n",
    "    dataset['TextNoMentions']=dataset['text'].apply(strip_all_entities)\n",
    "    return(list_trial)\n",
    "\n",
    "def hashtag_list(list_trial):\n",
    "    list_trial=list_trial.tolist()\n",
    "    result_trial = set(x for l in list_trial for x in l)\n",
    "    result_trial = list(map(lambda each:each.strip(\"#\"), result_trial))\n",
    "    result_set=set(result_trial)\n",
    "    print (result_set)\n",
    "    return (result_trial)   \n",
    "\n",
    "def list_to_dict(result_trial_list):\n",
    "    result_trial_list=[x.lower() for x in result_trial_list]\n",
    "    data=dict((x,result_trial_list.count(x)) for x in set(result_trial_list))\n",
    "    dataset_dict=pd.DataFrame.from_dict(data ,orient='index',columns=['Count'])\n",
    "    return(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_words_fraction - Fraction of words that are unique in a given text.\n",
    "\n",
    "import string\n",
    "def unique_word_fraction(row):\n",
    "    text = row['text']\n",
    "    text_splited = text.split(' ')  #spliting sentence into words with punctuation\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited] #remioving punctuation\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__() #lenght of the sentence meaning nu\n",
    "    unique_count = list(set(text_splited)).__len__() #gives unique words\n",
    "    return (unique_count/word_count)\n",
    "\n",
    "#stop_words - Fraction of stopwords present in a given text - Number of stopwords/Total words\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "def stopwords_count(row):\n",
    "    text = row['text'].lower()\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    stopwords_count = len([w for w in text_splited if w in eng_stopwords])\n",
    "    return (stopwords_count/word_count)\n",
    "\n",
    "#punctuations - Fraction of punctuation present in a given text - Number of puctuations/Total words\n",
    "def punctuations_fraction(row):\n",
    "    text = row['text']\n",
    "    char_count = len(text)\n",
    "    punctuation_count = len([c for c in text if c in string.punctuation])\n",
    "    return (punctuation_count/char_count)\n",
    "\n",
    "#number of characters present in a text i.e alphabet count\n",
    "def char_count(row):\n",
    "    return len(row['text'])\n",
    "\n",
    "def remove_noun(row):\n",
    "    text = row['text']\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    pos_list = nltk.pos_tag(text_splited)\n",
    "    \n",
    "    edited_sentence = [word for word,tag in pos_list if tag != 'NNP' and tag != 'NNPS' and tag != 'NN' and tag != 'NNS']\n",
    "\n",
    "    #noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n",
    "    \n",
    "    return (edited_sentence)\n",
    "\n",
    "# function to give us fraction of adjectives over total words in given text\n",
    "#JJ\tadjective\t'big'\n",
    "#JJR\tadjective, comparative\t'bigger'\n",
    "#JJS\tadjective, superlative\t'biggest'\n",
    "\n",
    "def remove_adj(row):\n",
    "    text = row['text']\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    pos_list = nltk.pos_tag(text_splited)\n",
    "    edited_sentence = [word for word,tag in pos_list if tag != 'JJ' and tag != 'JJR' and tag != 'JJS']\n",
    "\n",
    "    return (edited_sentence)\n",
    "\n",
    "# function to give us fraction of verbs over total words in given text\n",
    "#VB\tverb, base form\ttake\n",
    "#VBD\tverb, past tense\ttook\n",
    "#VBG\tverb, gerund/present participle\ttaking\n",
    "#VBN\tverb, past participle\ttaken\n",
    "#VBP\tverb, sing. present, non-3d\ttake\n",
    "#VBZ\tverb, 3rd person sing. present\ttakes   \n",
    "\n",
    "def remove_verbs(row):\n",
    "    text = row['text']\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    pos_list = nltk.pos_tag(text_splited)\n",
    "    edited_sentence = [word for word,tag in pos_list if tag != 'VB' and tag != 'VBD' and tag != 'VBG' and tag != 'VBN' and tag != 'VBP' and tag != 'VBZ']\n",
    "\n",
    "    return (edited_sentence)\n",
    "\n",
    "## function to give us fraction of adverbs over total words in given text\n",
    "#RB\tadverb\tvery, silently,\n",
    "#RBR\tadverb, comparative\tbetter\n",
    "#RBS\n",
    "def remove_adverbs(row):\n",
    "    text = row['text']\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    pos_list = nltk.pos_tag(text_splited)\n",
    "    edited_sentence = [word for word,tag in pos_list if tag != 'RR' and tag != 'RBR' and tag != 'RBS']\n",
    "\n",
    "    \n",
    "    return (edited_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_trial_1=column_text(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_trial_2=column_text(dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['text']=dataset['text'].apply(lambda row: row.encode().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joâ', 'DIY', 'EU', 'bitcoin', 'datascience', 'java', 'Enpiste', 'Csharâ', 'tecâ', 'Drones', 'javascript', 'science', 'GlobalDevelopment', 'inâ', 'matlab', 'reactjs', 'i4emploi', 'crypto', 'IoT', 'BigData', 'VR', 'MachineLearning', 'EarthObservation', 'influencer', 'SanFrancisco', 'ruby', 'infographic', 'tech', 'blockchain', 'competition', 'automation', 'AI', 'DataScience', 'Datascience', 'fintech', 'angular', 'python', 'Artiâ', 'SQYEmploi', 'vuejs', 'Mapping', 'perl'}\n"
     ]
    }
   ],
   "source": [
    "result_trial_2=hashtag_list(list_trial_1) #with hashtag machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joâ', 'DIY', 'EU', 'bitcoin', 'datascience', 'java', 'Enpiste', 'Csharâ', 'tecâ', 'Drones', 'javascript', 'science', 'GlobalDevelopment', 'inâ', 'matlab', 'reactjs', 'i4emploi', 'crypto', 'IoT', 'BigData', 'VR', 'MachineLearning', 'EarthObservation', 'influencer', 'SanFrancisco', 'ruby', 'infographic', 'tech', 'blockchain', 'competition', 'automation', 'AI', 'DataScience', 'Datascience', 'fintech', 'angular', 'python', 'Artiâ', 'SQYEmploi', 'vuejs', 'Mapping', 'perl'}\n"
     ]
    }
   ],
   "source": [
    "result_trial=hashtag_list(list_trial_1) #with hashtag AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_all_list=result_trial+result_trial_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>joâ</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigdata</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bitcoin</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drones</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sanfrancisco</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datascience</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>java</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tecâ</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vr</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>javascript</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machinelearning</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enpiste</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inâ</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matlab</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reactjs</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eu</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crypto</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i4emploi</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csharâ</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artiâ</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earthobservation</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>influencer</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruby</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diy</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infographic</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>globaldevelopment</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blockchain</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>competition</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automation</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mapping</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fintech</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angular</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iot</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>python</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqyemploi</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vuejs</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perl</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Count\n",
       "joâ                    2\n",
       "ai                     2\n",
       "bigdata                2\n",
       "bitcoin                2\n",
       "drones                 2\n",
       "sanfrancisco           2\n",
       "datascience            6\n",
       "java                   2\n",
       "tecâ                   2\n",
       "vr                     2\n",
       "javascript             2\n",
       "machinelearning        2\n",
       "enpiste                2\n",
       "science                2\n",
       "inâ                    2\n",
       "matlab                 2\n",
       "reactjs                2\n",
       "eu                     2\n",
       "crypto                 2\n",
       "i4emploi               2\n",
       "csharâ                 2\n",
       "artiâ                  2\n",
       "earthobservation       2\n",
       "influencer             2\n",
       "ruby                   2\n",
       "diy                    2\n",
       "infographic            2\n",
       "tech                   2\n",
       "globaldevelopment      2\n",
       "blockchain             2\n",
       "competition            2\n",
       "automation             2\n",
       "mapping                2\n",
       "fintech                2\n",
       "angular                2\n",
       "iot                    2\n",
       "python                 2\n",
       "sqyemploi              2\n",
       "vuejs                  2\n",
       "perl                   2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_to_dict(result_all_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(result_trial) == sorted(result_trial_2) #check whether list contains same value or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-648-222f4522150f>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-648-222f4522150f>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    outfile2.write('\\n')\u001b[0m\n\u001b[0m                        \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "''''with open('output_json_filtered.json', 'w') as outfile2:\n",
    "        filtered_keys = [\"tweet.created_at\", \"tweet.id\", \"tweet.user.followers_count\", \"tweet.user.friends_count\", \"tweet.retweet_count\", \"tweet.favorite_count\", \"tweet.text\"]\n",
    "        for filtered_keys in tweet:\n",
    "            json.dump(filtered_keys._json, outfile2, indent=2) \n",
    "            outfile2.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "citation:\n",
    "1. https://github.com/abdulfatir/twitter-sentiment-analysis/blob/master/code/preprocess.py\n",
    "2. https://stackoverflow.com/questions/8282553/removing-character-in-list-of-strings\n",
    "3. https://github.com/bear/python-twitter/blob/master/twitter/parse_tweet.py\n",
    "4. https://gist.github.com/dreikanter/2787146\n",
    "5. https://docs.python.org/3.4/howto/unicode.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = set(x for l in y for x in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result=list(result) #list with hashtags \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(map(handle_emojis, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "x = [''.join(c for c in s if c not in string.punctuation) for s in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#x=[y.encode('utf-8') for y in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(map(remove_emoji,x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
