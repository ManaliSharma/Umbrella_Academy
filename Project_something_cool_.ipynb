{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "import csv\n",
    "\n",
    "twitter_cred = dict()\n",
    "\n",
    "twitter_cred['CONSUMER_KEY'] = '17XHiftb4Iu5hVAIh4NacWj9S'\n",
    "twitter_cred['CONSUMER_SECRET'] = '8xAXhFsfnkiFjSFO5THqRO5J8x8lKSzC9J2GNPUSY7ZPb5z6Qv'\n",
    "twitter_cred['ACCESS_KEY'] = '1392379572-FInzXaziEqyNCGc8jqtU9LwnFFlHYRpjoihZpiX'\n",
    "twitter_cred['ACCESS_SECRET'] = 'JBDveSiUU6aXpwx8QkxYhXloDr9DyHn7ocqrqgWqwqctw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twitter_credentials.json', 'w') as secret_info:\n",
    "    json.dump(twitter_cred, secret_info, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twitter_credentials.json') as cred_data:\n",
    "    info = json.load(cred_data)\n",
    "    consumer_key = info['CONSUMER_KEY']\n",
    "    consumer_secret = info['CONSUMER_SECRET']\n",
    "    access_key = info['ACCESS_KEY']\n",
    "    access_secret = info['ACCESS_SECRET']\n",
    "\n",
    "# Create the api endpoint\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tweet_scrapped():\n",
    "    maximum_number_of_tweets_to_be_extracted = \\\n",
    "    tweet_number=int(input('Enter the number of tweets that you want to extract- '))\n",
    "    return tweet_number\n",
    "def Hashtag_used():\n",
    "    hashtag=str(input('Enter the hashtag you want to scrape- '))\n",
    "    return hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_tweets(hashtag,tweet_number):\n",
    "    all_the_tweets = [] \n",
    "    with open('output_json.json', 'w') as outfile:\n",
    "        \n",
    "        for tweet in tweepy.Cursor(api.search, q='#' + hashtag,rpp=100).items(tweet_number):\n",
    "            if (not tweet.retweeted) :\n",
    "                json.dump(tweet._json, outfile, indent=2) \n",
    "                outfile.write('\\n')\n",
    "                all_the_tweets.append(tweet)\n",
    "    #with open('tweets_with_hashtag_' + hashtag + '.txt', 'a') as the_file:\n",
    "        #the_file.write(str(tweet.text.encode('utf-8')) + '\\n')\n",
    "    #data=process_or_store(tweet._json)\n",
    "    #get_tweets()\n",
    "      \n",
    "\n",
    "    print ('Extracted ' + str(tweet_number) + ' tweets with hashtag #' + hashtag)\n",
    "    #print(all_the_tweets)\n",
    "    return all_the_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def out_tweets(tweets_scrapped):\n",
    "    \n",
    "    outtweets = [[tweet.id_str, tweet.created_at,tweet.text.encode('utf-8').decode(\"utf-8\"),tweet.user.name,tweet.user.screen_name,tweet.user.friends_count,tweet.user.followers_count,tweet.retweet_count,tweet.favorite_count] for tweet in tweets_scrapped]\n",
    "    return outtweets\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_tweets(outtweets,hashtag):\n",
    "    tweet_csv_name=' tweets with hashtag #' + hashtag+'.csv'\n",
    "    with open(tweet_csv_name, 'w', encoding='utf8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['id', 'created_at', 'text','user','screen_name','friends_count','followers_count','retweet_count','favorite_count'])\n",
    "    \n",
    "        writer.writerows(outtweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of tweets that you want to extract- 100\n"
     ]
    }
   ],
   "source": [
    "tweet_number=int(tweet_scrapped())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the hashtag you want to scrape- ML\n"
     ]
    }
   ],
   "source": [
    "hashtag=str(Hashtag_used())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 100 tweets with hashtag #ML\n"
     ]
    }
   ],
   "source": [
    "tweets_scrapped=scrap_tweets(hashtag,tweet_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "outtweets=out_tweets(tweets_scrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_tweets(outtweets,hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\" u\"\\U0001F600-\\U0001F64F\"  # emoticons \n",
    "                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                u\"\\U00002702-\\U000027B0\"\n",
    "                                u\"\\U000024C2-\\U0001F251\"\n",
    "                                u\"\\U0001f926-\\U0001f937\"\n",
    "                                u'\\U00010000-\\U0010ffff'\n",
    "                                u\"\\u200d\"\n",
    "                                u\"\\u2640-\\u2642\"\n",
    "                                u\"\\u2600-\\u2B55\"\n",
    "                                u\"\\u23cf\"\n",
    "                                u\"\\u23e9\"\n",
    "                                u\"\\u231a\"\n",
    "                                u\"\\u3030\"\n",
    "                                u\"\\ufe0f\"\n",
    "                                \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def dataframe(hashtag):\n",
    "    tweet_csv_name=' tweets with hashtag #' + hashtag+'.csv'\n",
    "    dataset=pd.read_csv(tweet_csv_name,encoding=\"ISO-8859-1\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset=dataframe('ML')\n",
    "#dataset_1=dataframe('Machinelearning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 9 columns):\n",
      "id                 100 non-null int64\n",
      "created_at         100 non-null object\n",
      "text               100 non-null object\n",
      "user               100 non-null object\n",
      "screen_name        100 non-null object\n",
      "friends_count      100 non-null int64\n",
      "followers_count    100 non-null int64\n",
      "retweet_count      100 non-null int64\n",
      "favorite_count     100 non-null int64\n",
      "dtypes: int64(5), object(4)\n",
      "memory usage: 7.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regexp = {\"HASHTAG\": r\"(#[\\w\\d]+)\"}\n",
    "regexp = dict((key, re.compile(value)) for key, value in regexp.items())\n",
    "\n",
    "def remove_hashtags(s):\n",
    "    return re.findall(regexp[\"HASHTAG\"], s)\n",
    "\n",
    "def extract_hash_tags(s):\n",
    "    my_set=set(part[1:] for part in s.split() if part.startswith('#'))\n",
    "    my_list = list(my_set)\n",
    "    return(my_list)\n",
    "\n",
    "def remove_hashtags_(x):\n",
    "    if '#' in x:\n",
    "        return str(x.replace('#',''))\n",
    "        \n",
    "\n",
    "def column_text(dataset):\n",
    "    #dataset['text_1']=dataset['text'].apply(lambda row: row.encode('utf-8'))\n",
    "    dataset['text_1']=dataset['text'].apply(lambda row: remove_emoji(row))\n",
    "    dataset['Hashtags']=dataset['text'].apply(lambda row: remove_hashtags(row))\n",
    "    list_trial=dataset['text'].apply(lambda row: remove_hashtags(row))\n",
    "    #dataset['Hashtags']=dataset['text'].apply(lambda row: extract_hash_tags(row))\n",
    "    dataset['TextNoHashtags']=dataset['text'].apply(remove_hashtags_)\n",
    "    return(list_trial)\n",
    "\n",
    "def hashtag_list(list_trial):\n",
    "    list_trial=list_trial.tolist()\n",
    "    result_trial = set(x for l in list_trial for x in l)\n",
    "    result_trial = list(map(lambda each:each.strip(\"#\"), result_trial))\n",
    "    result_set=set(result_trial)\n",
    "    print (result_set)\n",
    "    return (result_trial)   \n",
    "\n",
    "def list_to_dict(result_trial_list):\n",
    "    result_trial_list=[x.lower() for x in result_trial_list]\n",
    "    data=dict((x,result_trial_list.count(x)) for x in set(result_trial_list))\n",
    "    dataset_dict=pd.DataFrame.from_dict(data ,orient='index',columns=['Count'])\n",
    "    return(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_trial_1=column_text(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_trial_2=column_text(dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['text']=dataset['text'].apply(lambda row: row.encode().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Datascience', 'DataSecurity', 'Manu', 'analytics', 'socialmedia', 'Data', 'MachineLearning', 'Machinelearning', 'RPA', 'blockchain', 'IoT', 'Startup', 'Marketing', 'devsum19', 'GoogleNext19', 'judgment', 'banking', 'fintech', 'Tensorflow', 'dementia', 'MobileSecurity', 'Antivirus', 'Infographic', 'pijatsensual', 'tech', 'FinTech', 'Hyperconvergence', 'ArtificialIntelligence', 'ai', 'digitalmarketing', 'SaaS', 'PhDchat', 'books', 'Infographics', 'InfoSec', 'M', 'bo', 'industry40', 'MegaTrends', 'Blockchain', 'IIoT', 'Website', 'IOT', 'Fintech', 'datamanagemen', 'Mcafee', 'Privacy', 'newbie', 'cybersecurity', 'Keras', 'technology', 'startup', 'D', 'supplychain', 'healthcare', 'datathon', 'podcast', 'Tech', 'Artificialintelligence', 'CustomerExperience', 'mov', 'CyberSecurity', 'ComputerVision', 'coding', 'AcademicTwitter', 'Incubator', 'innovation', 'startups', 'C19TX', 'T', 'hacking', 'DL', 'NeuralNetworks', 'robots', 'Manufacturing', 'ML', 'contentmarketing', 'A', 'data', 'art', 'B', 'deepLearning', 'SplunkGovSummit', 'Seattle', 'CloudComputing', 'Healthcare', 'cloud', 'InternetOfT', 'AIToday', 'MedicalData', 'bigdata', 'SEO', 'ml', 'Se', 'Security', 'Toge', 'Startups', 'PredictiveMaintenance', 'Algorithms', 'socialmediamarketing', 'TensorFlow', 'DigitalTransformation', 'google', 'Software', 'DeepLearning', 'infographic', 'BigData', 'IndustrialIntelligence', 'MI', 'gr', 'Robo', 'Internet', 'CyberSecu', 'Azure', 'Technology', 'Jogja', 'pijatjogjakarta', 'EdgeComputing', 'finser', 'DataScience', 'AI', 'dl', 'Leben30', 'Ai', 'Investment', 'ethical', 'KI', 'KYC', 'mobile', 'TechnoGaud'}\n"
     ]
    }
   ],
   "source": [
    "result_trial_2=hashtag_list(list_trial_1) #with hashtag machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HuaweiBlog', 'Ro', 'Entrepreneurs', 'AIJobs', 'Cognilytica', 'VR', 'eMBB', 'MariaJohnsen', 'socialmedia', 'new', 'MachineLearning', 'RPA', 'algorithm', 'blockchain', 'IoT', 'Networking', 'patterns', 'Smart', 'NAB2019', 'recruitment', 'HPEpartner', 'EU', 'martech', 'digitalcommerce', 'datas', 'ChainL', 'insurance', 'technologies', 'FutureOfWork', 'crypto', 'deeplearning', 'sanidad', 'tech', 'FinTech', 'ArtificialIntelligence', 'SocialMedia', 'ai', 'corpgov', 'SmartCity', 's', 'Blockchain', 'Analytics', 'Salud', 'Fintech', 'Robotics', 'reality', 'cryptonews', 'ForeignAid', 'IVF', 'iiot', 'Autos', 'HRTech', 'HealthTech', 'Smarter', 'IBMStorage', 'technology', 'SMM', 'HM19', 'AR', 'AIforGood', 'ins', 'DriverlessCommute', 'innovation', 'blockchai', 'iot', 'Wearables', 'DL', 'Situational', 'land', 'WearableTech', 'Awareness', 'ML', 'Cloud', 'data', 'datascience', 'CloudComputing', 'cloud', 'Automotive', 'Know', 'DigitalHealth', 'bigdata', 'security', 'EVIEX', 'SEO', 'cryptocurrencies', 'ml', 'artificial', 'HR', 'marketing', 'ia', 'SDGs', 'IFTTT', 'Smartwatch', 'DeepLearning', 'EXCHANGE', 'SmartLiving', 'HPE', 'Engineering', 'BigData', 'Dentons', 'influen', 'ethicalAI', '4org', 'ethics', 'UbiNET', 'intelligence', 'DeepLearni', 'MariaJo', 'Shopping', 'Azure', 'waltonchain', 'live', 'autoindustry', 'robotics', 'AI', 'Business', 'DataScience', 'usecases', 'dl', 'robot', 'SC19', 'cars'}\n"
     ]
    }
   ],
   "source": [
    "result_trial=hashtag_list(list_trial_1) #with hashtag AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_all_list=result_trial+result_trial_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>autos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>splunkgovsummit</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machinelearning</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edgecomputing</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>digitaltransformation</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturing</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seo</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smartcity</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computervision</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>futureofwork</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cybersecu</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytics</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vr</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>socialmedia</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethicalai</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithm</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>digitalhealth</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blockchain</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shopping</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patterns</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foreignaid</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nab2019</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>devsum19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictivemaintenance</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recruitment</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judgment</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customerexperience</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ibmstorage</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gr</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4org</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethics</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>privacy</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intelligence</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technogaud</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aitoday</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kyc</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sc19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aijobs</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manu</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wearables</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waltonchain</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eviex</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seattle</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pijatjogjakarta</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autoindustry</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finser</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robotics</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usecases</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dl</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robot</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethical</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobile</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cars</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>antivirus</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Count\n",
       "autos                      1\n",
       "splunkgovsummit            1\n",
       "machinelearning            3\n",
       "edgecomputing              1\n",
       "digitaltransformation      1\n",
       "manufacturing              1\n",
       "seo                        2\n",
       "smartcity                  1\n",
       "computervision             1\n",
       "futureofwork               1\n",
       "cybersecu                  1\n",
       "analytics                  2\n",
       "vr                         1\n",
       "socialmedia                3\n",
       "ethicalai                  1\n",
       "new                        1\n",
       "algorithm                  1\n",
       "digitalhealth              1\n",
       "blockchain                 4\n",
       "shopping                   1\n",
       "patterns                   1\n",
       "know                       1\n",
       "hr                         1\n",
       "foreignaid                 1\n",
       "nab2019                    1\n",
       "devsum19                   1\n",
       "ar                         1\n",
       "predictivemaintenance      1\n",
       "recruitment                1\n",
       "judgment                   1\n",
       "...                      ...\n",
       "customerexperience         1\n",
       "ibmstorage                 1\n",
       "gr                         1\n",
       "4org                       1\n",
       "ethics                     1\n",
       "privacy                    1\n",
       "intelligence               1\n",
       "technogaud                 1\n",
       "aitoday                    1\n",
       "kyc                        1\n",
       "sc19                       1\n",
       "aijobs                     1\n",
       "software                   1\n",
       "manu                       1\n",
       "wearables                  1\n",
       "waltonchain                1\n",
       "live                       1\n",
       "eviex                      1\n",
       "seattle                    1\n",
       "pijatjogjakarta            1\n",
       "autoindustry               1\n",
       "finser                     1\n",
       "robotics                   2\n",
       "usecases                   1\n",
       "dl                         4\n",
       "robot                      1\n",
       "ethical                    1\n",
       "mobile                     1\n",
       "cars                       1\n",
       "antivirus                  1\n",
       "\n",
       "[193 rows x 1 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_to_dict(result_all_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(result_trial) == sorted(result_trial_2) #check whether list contains same value or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-648-222f4522150f>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-648-222f4522150f>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    outfile2.write('\\n')\u001b[0m\n\u001b[0m                        \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "''''with open('output_json_filtered.json', 'w') as outfile2:\n",
    "        filtered_keys = [\"tweet.created_at\", \"tweet.id\", \"tweet.user.followers_count\", \"tweet.user.friends_count\", \"tweet.retweet_count\", \"tweet.favorite_count\", \"tweet.text\"]\n",
    "        for filtered_keys in tweet:\n",
    "            json.dump(filtered_keys._json, outfile2, indent=2) \n",
    "            outfile2.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "citation:\n",
    "1. https://github.com/abdulfatir/twitter-sentiment-analysis/blob/master/code/preprocess.py\n",
    "2. https://stackoverflow.com/questions/8282553/removing-character-in-list-of-strings\n",
    "3. https://github.com/bear/python-twitter/blob/master/twitter/parse_tweet.py\n",
    "4. https://gist.github.com/dreikanter/2787146\n",
    "5. https://docs.python.org/3.4/howto/unicode.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = set(x for l in y for x in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result=list(result) #list with hashtags \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(map(handle_emojis, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "x = [''.join(c for c in s if c not in string.punctuation) for s in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#x=[y.encode('utf-8') for y in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(map(remove_emoji,x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
