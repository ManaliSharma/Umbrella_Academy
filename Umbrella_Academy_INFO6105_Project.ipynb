{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Social Butterfly - Umbrella Academy</p>\n",
    "![title](Umbrella_Academy_INFO6105_Project\\Images\\Title_Images\\social-butterfly.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> 1.0 Abstract </p> <a id='abstract'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social Butterfly is a social media engagement software project which is a part of the NEU AI Skunkworks Project team. We propose to create a software that performs the task of content optimization and then publishes the same optimized content on one or more social media platform (viz. Twitter, Slack, Instagram etc.).\n",
    "\n",
    "Publishing content on social media is the fastest way to reach audience such that the content that should be posted and the timing at which it should be posted is of prime importance. Finding new content is easier these days but finding content with good data quality is incredibly hard. Also, then uniqueness of content poses a huge problem, because generating unique content in this era of duplication is like finding a needle in a haystack. And let's say even when you are able to create it and that doesn’t get popular and is not ranked well, is a wasted expense. However, producing a blog post which becomes popular and ranks highly is a wise investment. Repeating the same process will give you a consistent basis for a serious competitive advantage. Most people feel that optimizing content manually is a tedious and time consuming process. But it doesn’t have to be that way.  \n",
    "\n",
    "And that's how our idea builds, we are aiming to create content with good data quality in a given time frame which can become popular as it gets posted and majorly adds a x-factor to one's social media handle. We have tried incorporating content optimization into content creation workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PS-** We have done this project in 3 parts, and all 3 parts are divided into seperate notebooks. And these individual notebooks contain the whole code and documentation of the entire part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Collecting Metadata**:- In lay man's term for creating content on particular topic we need to know what is the present basis of it. And for the same reason this notebook comes handy. In this notebook we tackle with the task of Metadata collection . Metadata is nothing but the keywords which shows relevance of a topic and by which searching about the topic becomes easier. We have chosen to collect metadata in form of hashtags from twitter as it is easy to scrape data from twitter with the help of Python library like Tweepy. We save the scraped tweets in json file and then convert it to a csv file. This csv file is then loaded as a pandas dataframe, and we extract hashtags from the tweets. Then we plot these hashtags with their count to see their frequency of occurence. This gives us some good idea as to which hashtags should we use while scraping content from twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **LSTM Modeling**:- Now that we have scrapped metadata in form of hashtags in Collecting Metadata (Part 1) ,we will be scrapping relevant data using metadata i.e hashtags from twitter by means of a real time streaming listener. For this data being scrapped we won't be using retweets and the conditional requirement is that dataset should be large enough in order fo us to build the model.And then we will be employing various machine learning algorithm for content generation. Mostly we will be using LSTM for generation if content and we will be tweaking the same to get a better model which yields better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Sentiment Analysis**:- Generating content was a primary task for this project , but only creation of it is not enough. We also need to know how it influences people and creates what sort of impact on social media networks. And that's when sentiment analysis comes in play. In this notebook , we have performed sentiment analysis on the real time tweets (newly generated not the one we did our modeling on in previous notebook), that we collected using stream listener. And we found sentiment for each tweet and compared the tweets on the basis of the sentiments recieved. We used textblob to find the context of the inputted document we formed using tweets . After this we used a gensim word2vec model to find words with similar meaning and then we employed LSTM modeling for finding the sentiment of inputted texts that is being employed on text generated (extracted from part 2 i.e LSTM modeling of the project) in order to see what influence it will have on people. The following notebook can also be used for finding influencers that has most impact on twitter over all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our project aims to create a evolving model which optimizes the content for social platform  by experimenting on other available platforms like Twitter , Instagram etc. We have done so by using various machine learning techniques like LSTM etc and open source libraries like textblob, gensim, word2vec to make our model.**\n",
    "\n",
    "Relevant information about each topic can be found in each notebook which is specified in the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Index </p>\n",
    "- # 1 [Abstract](#abstract)\n",
    "- # 2 [Collecting Metadata](./Umbrella_Academy_INFO6105_Project/Umbrella_Academy_INFO6105_Collecting_Metadata.ipynb)\n",
    "- # 3 [LSTM Modeling](./Umbrella_Academy_INFO6105_Project/Umbrella_Academy_INFO6105_LSTM_Model.ipynb)\n",
    "- # 4 [Sentiment Analysis](./Umbrella_Academy_INFO6105_Project/Umbrella_Academy_INFO6105_Sentiment_Analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Conclusion </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we can see from the graph above AI, datascience, ML, machinelearning, bigdata, artificialintelligence and deeplearning were the top 7 hashtags in the observed dataset.\n",
    "\n",
    "2. We extract the hashtag count using two methods, manually and with using the library advertools. As we can see, the top 7 hashtags are same in both the methods, although a count may differ a bit. This may be due to the fact that library extracting some deep hidden hashtags. We don't know the exact working of the library so this is meagre speculation.\n",
    "\n",
    "3. The difference in count is not very significant, and we are more interested in the trend of the use of hashtag, rather than the actual count.\n",
    "\n",
    "4. We are not interested in the most accurate (classification accuracy) model of the training dataset. The model is the one that predicts each character and word in the training dataset perfectly. Instead we are interested in a generalization of the dataset that minimizes the chosen loss function. We are seeking a balance between generalization and overfitting but short of memorization.\n",
    "\n",
    "5. As of now we are able to generate text, which seems much more gibberish but is making sense in some parts, the text generated can be tuned by adding different dense layers , building complex neuron network and by increasing epochs and decreasing the batch size.\n",
    "\n",
    "6. Model can also perform well , if we take a large vocab size and our input seed lenght is good enough, although time consuming, but output can vary differently if we experiment it in that way.\n",
    "\n",
    "7. Adding weights to model with better accuracy results results in making a model (deeper and denser one better), we can keep on iterating that ways.\n",
    "\n",
    "8. The collected data can be tested upon to find its sentiment by our third project notebook (sentiment analysis part 3)\n",
    "\n",
    "9. As we scrapped the data for given list of hashtags, we found that the sentiment is mostly positive or neutral than negative.\n",
    "\n",
    "10. When we employed text blob, it was giving us 70% accuracy in aspect of knowing the data.\n",
    "\n",
    "11. We employed Gensim, and with it's help we were able to find similar meaning words i.e. 'machinelearning' returned python, datascience, deep learing and so on.\n",
    "\n",
    "12. The model on the test set of 3 class sentiment classification provides a result of 53.7 %accuracy.\n",
    "\n",
    "13. We modeled the data and achieved 50% accuracy, but that was not the aim, we are seeking a balance between generalization and on overfitting and getting a 50% accuracy is also a good enough measure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Contribution<p><a id='Contribution'></a>\n",
    "\n",
    "    \n",
    "- Code by self : 75%\n",
    "- Code from external Sources : 25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">  License<p><a id='License'></a>\n",
    "Copyright (c) 2019 Rushabh Nisher, Manali Sharma\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
